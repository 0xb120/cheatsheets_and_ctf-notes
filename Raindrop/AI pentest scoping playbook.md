---
raindrop_id: 1450878276
raindrop_highlights:
  6932f25ac61f4cfe1c918283: fefa05bf540490c97aca6b1833c10eb1
  6932f26dcbea73cbcc49d111: ac21a43c2f3d59607edbeca33ea14568
  6932f301dea0cc0bdacf6c53: 93ce2ed9afd3e55319b4b23ab1962f87
  6932f328cbe58c5707056e0e: 50e51ff55a90b9d0d824572b34f52db0
  6932f34dd9344396f73e0f0e: 40bd979e8fb00b99e48d5ca259543574
  6932f35cc61f4cfe1c91c98d: febe0c93a37145d2c87ea062c6ee0d0d
  6932f37416ef53efb5d34468: b223046800a325d674e97d65371843a6
  6932f3924c51a34b840e5415: a62c91d46dca5a7ec7b056f1105ac69e
  6932f3bfcbe58c5707059657: 1dceddf6f68da3d18cf168f9101e37fc
  6932f69da06963483040afc1: f22097d18b850e55745ff7a7540a6bb1
  6932f6dae1dbc641a5df1e50: 7878998ed201ee077e995c623683627b
  6932f6e79fae5c8ff0e3797a: 37e0d34c1f711016e0137e667f2e3170
  6932f707e1dbc641a5df2a82: e2b56dfac155c8393aaba464d0e4ae35
  6932f70df4b83213eaff28fd: 5dc47a731c3ea65abfadb1bdb661e218
  6932f729e29bbbc7ef9e0de4: 96d865c3d3ec6b9725f1fa8df9ce3b5e
  6932f77eb1ef82362b658f9a: 21acf6bcc4a52dcfcd81718c4e3689ad
title: "AI pentest scoping playbook"

description: |-
  Organizations are throwing absurd amount of money at "AI red teams" who run a few prompt injection tests, declare victory, and cash checks. Security consulta...

source: https://devansh.bearblog.dev/ai-pentest-scoping/

created: 1764101367612
type: article
tags:
  - "_index"

 
  - "AI" 
  - "tech-blog"

---
# AI pentest scoping playbook

![](https://bear-images.sfo2.cdn.digitaloceanspaces.com/herman-1683556668-0.png)

> [!summary]
> Organizations are throwing absurd amount of money at "AI red teams" who run a few prompt injection tests, declare victory, and cash checks. Security consulta...





AI pentest scoping playbook
Organizations are throwing absurd amount of money at "AI red teams" who run a few prompt injection tests, declare victory, and cash checks.
This guide exists because the current state of AI security testing is dangerously inadequate. The attack surface is massive. The risks are novel. The methodologies are immature. And the consequences of getting it wrong are catastrophic.
What follows is what I wish every CISO, security lead, and AI team lead understood before they scoped their next AI security engagement.
Why AI Pentesting Is Different
AI systems break all of that.

The system output is non-deterministic. You can't write a test case that says "given input X, expect output Y" because the model might generate something completely different next time. This makes reproducibility, the foundation of security testing, fundamentally harder.
Also, the attack surface is layered and interconnected. You're not just testing an application. You're testing a model (which might be proprietary and black-box), a data pipeline (which might include RAG, vector stores, and real-time retrieval), integration points (APIs, plugins, browser tools), and the infrastructure underneath (cloud services, containers, orchestration).
Novel attack classes exist that don't map to traditional vuln categories.
AI systems are probabilistic, data-dependent, and constantly evolving. A model that's safe today might become unsafe after fine-tuning. A RAG system that's secure with Dataset A might leak PII when Dataset B is added. An autonomous agent that behaves correctly in testing might go rogue in production when it encounters edge cases.
OWASP LLM Top 10 is your Baseline

The OWASP Top 10 for LLM Applications is the closest thing we have to a standardized framework for AI security testing. If you're scoping an AI engagement and you haven't mapped every item in this list to your test plan, you're doing it wrong.
The OWASP LLM Top 10 is valuable, but it's not comprehensive. Here's what's missing in it:

AI Safety Risks

Safety is not the same as security. Unsafe AI systems cause real harm, and that should be scope for red teaming.

Can the model be made to behave in ways that violate its stated values? Can adversaries bypass constitutional AI techniques like Anthropic's Claude? Does the model exhibit or amplify demographic biases across different groups? Can the model be tricked into generating illegal, dangerous, or abusive content? Can the model lie, manipulate, or deceive users? These aren't just ethics issues. They're legal risks under GDPR, EEOC, and other regulations.
Adversarial Machine Learning

Traditional adversarial ML attacks apply to AI systems.

Can adversaries craft inputs that cause misclassification?
Can adversaries reconstruct training data from model outputs?
Can adversaries steal model weights through repeated queries?
Can adversaries determine if specific data was in the training set? - Does the model have hidden backdoors that trigger on specific inputs?
Multimodal Risks

If your AI system handles multiple modalities (text, images, audio, video), you have additional attack surface.
Privacy and Compliance

AI systems must comply with GDPR, HIPAA, CCPA, and other regulations.
Scoping Questions

Before you write your scope document, answer every single one of these questions. If you can't answer them, you don't understand your system well enough to scope a meaningful AI security engagement.

About the Model
What base model are you using (GPT-4, Claude, Llama, Mistral, custom)?
Is the model proprietary (OpenAI API) or open-source?
Have you fine-tuned the base model? On what data?
Have you applied instruction tuning, RLHF, or other alignment techniques?
How is the model deployed (API, on-prem, container, serverless)?
Do you have access to model weights?
Can testers query the model directly, or only through your application?
Are there rate limits? What are they?
What's the model's context window size?
Does the model support function calling or tool use?
Is the model multimodal (vision, audio, text)?
Are you using multiple models in ensemble or orchestration?
About Data
Where did training data come from (public, proprietary, scraped, licensed)?
Was training data curated or filtered? How?
Is training data in scope for poisoning tests?
Are you using RAG (Retrieval-Augmented Generation)?
If RAG: What's the document store (vector DB, traditional DB, file system)?
If RAG: How are documents ingested? Who controls ingestion?
If RAG: Can testers inject malicious documents?
If RAG: How is retrieval indexed and searched?
Do you pull real-time data from external sources (APIs, databases)?
How is input data preprocessed and sanitized?
Is user conversation history stored? Where? For how long?
Can users access other users' data?
About Application Integration
How do users interact with the model (web app, API, chat interface, mobile app)?
What authentication mechanisms are used (OAuth, API keys, session tokens)?
What authorization model is used (RBAC, ABAC, none)?
Are there different user roles with different permissions?
Is there rate limiting? At what levels (user, IP, API key)?
Are inputs and outputs logged? Where?
Who has access to logs?
Are logs encrypted at rest and in transit?
How are errors handled? Are error messages exposed to users?
Are there webhooks or callbacks that the model can trigger?
About Plugins and Tool Use
Can the model call external APIs? Which ones?
Can the model execute code? In what environment?
Can the model browse the web?
Can the model read/write files?
Can the model access databases?
What permissions do plugins have?
How are plugin outputs validated before use?
Can users add custom plugins?
Are plugin interactions logged?
About Autonomous Agents
Do you have autonomous agents that plan and execute multi-step tasks?
What tools can agents use?
Can agents spawn other agents?
Do agents have persistent memory? Where is it stored?
How are agent goals and constraints defined?
Can agents access sensitive resources (DBs, APIs, filesystems)?
Can agents escalate privileges?
Are there kill-switches or circuit breakers for agents?
How is agent behavior monitored?
About Infrastructure
What cloud provider(s) are you using (AWS, Azure, GCP, on-prem)?
Are you using containers (Docker)? Orchestration (Kubernetes)?
Where are model weights stored? Who has access?
Where are API keys and secrets stored?
Are secrets in environment variables, config files, or secret managers?
How are dependencies managed (pip, npm, Docker images)?
Have you scanned dependencies for known vulnerabilities?
How are model updates deployed? What's the CI/CD pipeline?
Who can deploy model updates?
Are there staging environments separate from production?
About Safety and Alignment
What safety mechanisms are in place (content filters, refusal training, constitutional AI)?
Have you red-teamed for jailbreaks?
Have you tested for bias across demographic groups?
Have you tested for harmful content generation?
Do you have human-in-the-loop review for sensitive outputs?
What's your incident response plan if the model behaves unsafely?
About Testing Boundaries
Can testers attempt to jailbreak the model?
Can testers attempt prompt injection?
Can testers attempt data extraction (training data, PII)?
Can testers attempt model extraction or inversion?
Can testers attempt DoS or resource exhaustion?
Can testers poison training data (if applicable)?
Can testers test multi-turn conversations?
Can testers test RAG document injection?
Can testers test plugin abuse?
Can testers test agent privilege escalation?
Are there any topics, content types, or test methods that are forbidden?
What's the escalation process if critical issues are found during testing?
About Compliance and Legal
What regulations apply (GDPR, HIPAA, CCPA, FTC, EU AI Act)?
Do you process PII? What types?
Do you have data processing agreements with model providers?
Do you have the legal right to test this system?
Are there export control restrictions on the model or data?
What are the disclosure requirements for findings?
What's the confidentiality agreement for testers?

If you can answer all these questions, you're ready to scope. If you can't, you're not.
Writing the Scope Document

Your AI pentest engagement scope document needs to be more detailed than a traditional pentest scope.

Section 1: Executive Summary

One-paragraph description of the AI system. Business objectives (compliance, pre-launch validation, continuous assurance, incident response). Top 3-5 risks that drive the engagement. What does "passing" look like?

Section 2: System Architecture

Include an architectural diagram showing everything: model, data pipelines, APIs, infrastructure, third-party services. List every testable component with owner, version, and deployment environment. Document how data moves through the system, from user input to model output to downstream consumers. Identify where data crosses trust boundaries.

Section 3: In-Scope Components

Be exhaustive. List models, APIs, data stores, integrations, infrastructure, applications. For each component, specify access credentials testers will use, environments that are in scope, testing windows if limited, and rate limits or usage restrictions.

Section 4: Attack Vectors and Test Cases

Map every OWASP LLM Top 10 item to specific test cases. For LLM01 prompt injection: test direct instruction override, indirect injection via RAG documents, multi-turn conditioning, system prompt extraction, jailbreak techniques, cross-turn memory poisoning.

Include specific threat scenarios: Can an attacker leak other users' conversation history? Can an attacker extract training data containing PII? Can an attacker bypass content filters to generate harmful instructions?

Section 5: Out-of-Scope Components

Explicitly list what's NOT being tested: production environments if testing only staging, physical security, social engineering of employees, third-party SaaS providers you don't control, specific attack types if any are prohibited.

Section 6: Testing Methodology

List specific tools: Promptfoo for LLM fuzzing, Garak for red teaming, PyRIT for adversarial prompting, ART for ML attacks, custom scripts for specific attack vectors, traditional tools like Burp Suite for infrastructure.

Testing techniques: prompt injection testing, jailbreak attempts, data extraction attacks, model inversion, membership inference, evasion attacks, RAG poisoning, plugin abuse, agent privilege escalation, infrastructure scanning.

Section 7: Rules of Engagement

All testing must be explicitly authorized in writing with names, signatures, dates. No attempts at physical harm, financial fraud, or illegal content generation unless explicitly scoped for red teaming. Critical findings must be disclosed immediately via designated channel. Standard findings can wait for formal report.

Testers must not exfiltrate user data, training data, or model weights except as explicitly authorized for demonstration purposes. All test data must be destroyed post-engagement. Testing must comply with all applicable laws and regulations.

Section 8: Deliverables

Technical report with detailed findings, severity ratings, reproduction steps, evidence, and remediation guidance. Executive summary for business leadership. Updated threat model. Retest availability confirmation.

Section 9: Timeline and Contacts

Start date, end date, report delivery date, retest window. Key contacts: engagement lead, technical point of contact, escalation contact for critical findings, legal contact for scope questions.

That's your scope document. It should be 10-20 pages. If it's shorter, you're missing things.